import os
import streamlit as st
import pandas as pd
import pyodbc
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import re
import datetime
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.decomposition import LatentDirichletAllocation
import numpy as np

# Descargar recursos de NLTK (solo la primera vez)
nltk.download('stopwords')
nltk.download('vader_lexicon')

# --------------------------------
# 1. Conexi√≥n a SQL Server usando variables de entorno
# --------------------------------
@st.cache_resource
def get_connection():
    server   = os.getenv("DB_SERVER")
    database = os.getenv("DB_NAME")
    user     = os.getenv("DB_USER")
    password = os.getenv("DB_PASSWORD")

    if not all([server, database, user, password]):
        st.error("üî¥ Faltan variables de entorno para la conexi√≥n SQL Server. Revisa los Secrets.")
        st.stop()

    conn_str = (
        "DRIVER={ODBC Driver 17 for SQL Server};"
        f"SERVER={server};"
        f"DATABASE={database};"
        f"UID={user};"
        f"PWD={password}"
    )
    return pyodbc.connect(conn_str)

# --------------------------------
# 2. Cargar y procesar Posts
# --------------------------------
@st.cache_data
def load_posts():
    conn = get_connection()
    query = "SELECT * FROM Posts"
    df = pd.read_sql(query, conn)
    df['Fecha'] = pd.to_datetime(df['Fecha'])
    # Asegurarse de que Post_ID se maneje como string para buscar con guiones bajos
    df['Post_ID'] = df['Post_ID'].astype(str)
    if 'Reacciones' not in df.columns:
        df['Reacciones'] = 0
    return df[['Post_ID', 'Fecha', 'Autor', 'Mensaje', 'Reacciones']].copy()

# --------------------------------
# 3. Cargar y procesar Comments
# --------------------------------
@st.cache_data
def load_comments():
    conn = get_connection()
    query = "SELECT * FROM Comments"
    df_c = pd.read_sql(query, conn)
    df_c['Fecha'] = pd.to_datetime(df_c['Fecha'])
    df_c['Post_ID'] = df_c['Post_ID'].astype(str)  # convertir a string tambi√©n
    if 'Reacciones' not in df_c.columns:
        df_c['Reacciones'] = 0
    return df_c[['Comment_ID', 'Post_ID', 'Autor', 'Mensaje', 'Fecha', 'Reacciones']].copy()

# --------------------------------
# 4. Cargar DataFrames
# --------------------------------
df_posts = load_posts()
df_comments = load_comments()

# --------------------------------
# 5. Funci√≥n para categorizar Posts con diccionario ampliado
# --------------------------------
@st.cache_data
def categorize_posts(df):
    categoria_dict = {
        "Pol√≠tica": [
            "gobierno", "elecci√≥n", "parlamento", "ley", "ministro",
            "presidente", "alcalde", "congreso", "diputado", "senador",
            "votaci√≥n", "campa√±a", "coalici√≥n", "partido", "oposici√≥n",
            "constituci√≥n","Gonzalo Winter","Gabriel Boric","cosejal","alcalde","luis malla","gobernador","jorge diaz",
            "republicanos","liberales","partido","toha" 
        ],
        "Econom√≠a": [
            "econom√≠a", "inflaci√≥n", "PIB", "mercado", "impuestos",
            "√≠ndice", "inversi√≥n", "banco", "d√≥lar", "bolsa",
            "desempleo", "subsidio", "exportaci√≥n", "importaci√≥n",
            "crisis econ√≥mica", "finanzas"
        ],
        "Salud": [
            "hospital", "vacuna", "pandemia", "covid", "salud",
            "paciente", "enfermero", "doctor", "ministerio de salud",
            "enfermedad", "vacunaci√≥n", "epidemia", "emergencia sanitaria",
            "consultorio", "medicamento"
        ],
        "Educaci√≥n": [
            "escuela", "universidad", "colegio", "profesor", "alumno",
            "matr√≠cula", "prueba", "examen", "curr√≠culo", "docente",
            "taller", "bachillerato", "educaci√≥n superior", "pedagog√≠a"
        ],
        "Seguridad": [
            "delito", "polic√≠a", "robo", "homicidio", "seguridad ciudadana",
            "justicia", "tribunal", "carabinero", "prisi√≥n", "recluso",
            "vigilancia", "crimen", "violencia", "menor infractor",
            "accidente","cerro chu√±o","delicuentes","delicuentes","ladron"
        ],
        "Medio Ambiente": [
            "contaminaci√≥n", "reciclaje", "cambio clim√°tico", "deforestaci√≥n",
            "biodiversidad", "remanente forestal", "vertedero", "energ√≠as renovables",
            "agua", "basura", "aire", "sustentabilidad", "biocombustible"
        ],
        "Tecnolog√≠a": [
            "tecnolog√≠a", "software", "hardware", "inteligencia artificial",
            "innovaci√≥n", "start-up", "aplicaci√≥n", "redes sociales",
            "ciberseguridad", "blockchain", "criptomoneda", "internet",
            "transformaci√≥n digital"
        ],
        "Cultura": [
            "museo", "exposici√≥n", "teatro", "cine", "libro", "arte",
            "concierto", "festival", "danza", "pintura", "escultura",
            "fotograf√≠a", "m√∫sica", "teatro comunitario", "patrimonio"
        ],
        "Deportes": [
            "f√∫tbol", "tenis", "ol√≠mpico", "messi", "torneo", "jugador",
            "selecci√≥n", "liga", "partido", "entrenador", "marat√≥n",
            "juegos", "deporte femenino", "estadio"
        ],
        "Internacional": [
            "onu", "relaciones exteriores", "conflicto", "guerra",
            "tratado", "acuerdo", "embargo", "diplom√°tico", "visita oficial",
            "s√°nchez", "biden", "putin", "xi jinping", "ue", "otan"
        ],
        "Sociedad": [
            "comunidad", "vecino", "manifestaci√≥n", "protesta",
            "derechos humanos", "inmigraci√≥n", "desigualdad", "pobreza",
            "migrante", "ni√±ez", "juventud", "embarazo adolescente",
            "movilidad social"
        ],
        "Otros": []
    }

    def asignar_categoria(texto):
        if not isinstance(texto, str):
            return "Otros"
        texto_min = texto.lower()
        for cat, keywords in categoria_dict.items():
            for kw in keywords:
                pattern = rf"\b{re.escape(kw)}\b"
                if re.search(pattern, texto_min):
                    return cat
        return "Otros"

    df['Categoria'] = df['Mensaje'].apply(asignar_categoria)
    return df

df_posts = categorize_posts(df_posts)

# 5.1 Asignar categor√≠a a Comments (basado en categor√≠a del Post padre)
df_comments = df_comments.merge(df_posts[['Post_ID', 'Categoria']], on='Post_ID', how='left')
df_comments['Categoria'] = df_comments['Categoria'].fillna("Otros")

# --------------------------------
# 6. Sidebar: selecci√≥n de dataset, categor√≠a, rango de fechas, n-gramas y opciones
# --------------------------------
st.sidebar.header("Filtros y Opciones")

# 6.1. Selector de dataset
dataset_option = st.sidebar.selectbox("Seleccionar dataset", ["Posts", "Comments"])

# 6.2. Selector de categor√≠a
if dataset_option == "Posts":
    categorias = ["Todas"] + sorted(df_posts["Categoria"].unique().tolist())
else:
    categorias = ["Todas"] + sorted(df_comments["Categoria"].unique().tolist())
selected_category = st.sidebar.selectbox("Categor√≠a", categorias)

# 6.3. Slider de rango de fechas: iniciar el 1 de marzo de 2025 hasta hoy
fixed_start = datetime.date(2025, 3, 1)
max_fecha = datetime.date.today()
fecha_inicio, fecha_fin = st.sidebar.slider(
    "Rango de fechas",
    min_value=fixed_start,
    max_value=max_fecha,
    value=(fixed_start, max_fecha),
    format="YYYY-MM-DD"
)

# 6.4. Selector de tipo de n-grama (ahora incluye Pentagramas)
ngram_option = st.sidebar.selectbox(
    "Tipo de n-grama",
    ["Unigramas", "Bigramas", "Trigramas", "Cuatrigramas", "Pentagramas"]
)
if ngram_option == "Unigramas":
    ngram_range = (1, 1)
elif ngram_option == "Bigramas":
    ngram_range = (2, 2)
elif ngram_option == "Trigramas":
    ngram_range = (3, 3)
elif ngram_option == "Cuatrigramas":
    ngram_range = (4, 4)
else:  # Pentagramas
    ngram_range = (5, 5)

# 6.5. Checkbox para usar m√©tricas de Likes
use_likes = st.sidebar.checkbox("Mostrar m√©trica de Likes (sumatoria)", value=False)

# 6.6. Input para b√∫squeda de palabras clave en Posts
post_search = st.sidebar.text_input("Buscar en Posts", "")

# --------------------------------
# 7. Filtrado seg√∫n selecci√≥n para Posts o Comments
# --------------------------------
if dataset_option == "Posts":
    df = df_posts.copy()
else:
    df = df_comments.copy()

# Filtrar por categor√≠a
if selected_category != "Todas":
    df = df[df["Categoria"] == selected_category]

# Filtrar por fechas
mask = (df['Fecha'].dt.date >= fecha_inicio) & (df['Fecha'].dt.date <= fecha_fin)
df = df.loc[mask]

# Filtrar por palabra clave en Posts si aplica
if dataset_option == "Posts" and post_search.strip() != "":
    df = df[df['Mensaje'].str.contains(post_search, case=False, na=False)]

# --------------------------------
# 8. KPI Dashboard
# --------------------------------
st.title("An√°lisis Avanzado de N-gramas, Sentimiento y Temas con KPIs")

col1, col2, col3, col4 = st.columns(4)
col1.metric("Total Registros", f"{len(df)}")
avg_reacts = df["Reacciones"].mean() if "Reacciones" in df.columns else 0
col2.metric("Reacciones Promedio", f"{avg_reacts:.2f}")

if dataset_option == "Comments":
    sia = SentimentIntensityAnalyzer()
    df['compound_tmp'] = df['Mensaje'].fillna("").apply(
        lambda t: sia.polarity_scores(str(t))['compound']
    )
    avg_sent = df['compound_tmp'].mean()
    if avg_sent < -0.1:
        status = "üî¥ Negativo"
    elif avg_sent > 0.1:
        status = "üü¢ Positivo"
    else:
        status = "üü† Neutral"
    col3.metric("Sentimiento Promedio", f"{avg_sent:.2f}", status)
else:
    col3.metric("Sentimiento Promedio", "N/A")

num_cat = df['Categoria'].nunique() if 'Categoria' in df.columns else 0
col4.metric("Categor√≠as Activas", f"{num_cat}")

# --------------------------------
# 9. Buscador de Post por ID (solo Posts)
# --------------------------------
if dataset_option == "Posts":
    st.subheader("Buscar Post por ID")
    post_id_input = st.text_input("Introduce Post ID", "")
    if post_id_input.strip() != "":
        # No convertir a int; buscar como cadena
        pid_str = post_id_input.strip()
        df_found = df[df['Post_ID'] == pid_str]
        if not df_found.empty:
            post_row = df_found.iloc[0]
            st.markdown(f"**Post encontrado:**")
            st.markdown(f"- ID: {post_row['Post_ID']}")
            st.markdown(f"- Autor: {post_row['Autor']}")
            st.markdown(f"- Fecha: {post_row['Fecha'].date()}")
            st.markdown(f"- Mensaje: {post_row['Mensaje']}")
            # Construir un enlace de ejemplo; aj√∫stalo seg√∫n tu plataforma real
            ejemplo_url = f"https://facebook.com/{post_row['Post_ID']}"
            st.markdown(f"[Ir al Post]({ejemplo_url})", unsafe_allow_html=True)
        else:
            st.write("No se encontr√≥ ning√∫n post con ese ID en los resultados actuales.")

# --------------------------------
# 10. Si buscamos en Posts, mostrar n‚Äêgrams de Comments relacionados
# --------------------------------
if dataset_option == "Posts" and post_search.strip() != "":
    st.header("N‚Äêgramas de Comentarios Relacionados a los Posts Encontrados")
    post_ids = df['Post_ID'].unique().tolist()
    df_related_comments = df_comments[df_comments['Post_ID'].isin(post_ids)].copy()
    if not df_related_comments.empty:
        textos_rel = df_related_comments['Mensaje'].dropna().astype(str)
        base_stop = list(stopwords.words("spanish"))
        extra_stop = [
            "http", "https", "www", "com", "org", "net", "ftp",
            "https://", "http://", "://", "url", "rt"
        ]
        spanish_stopwords = base_stop + extra_stop

        @st.cache_data
        def gen_ngrams_relat(text_series: pd.Series, ngram_range=(1, 1)):
            vectorizer = CountVectorizer(
                ngram_range=ngram_range,
                stop_words=spanish_stopwords
            )
            X = vectorizer.fit_transform(text_series)
            ngrams = vectorizer.get_feature_names_out()
            counts = X.sum(axis=0).A1
            return pd.DataFrame({"ngram": ngrams, "count": counts}).sort_values(by="count", ascending=False)

        df_ngrams_rel = gen_ngrams_relat(textos_rel, ngram_range=ngram_range)
        st.subheader(f"Top N‚Äêgramas en Comentarios relacionados ({ngram_option})")
        st.dataframe(df_ngrams_rel.head(50), use_container_width=True)
    else:
        st.write("No hay comentarios relacionados con los posts encontrados.")

# --------------------------------
# 11. Detecci√≥n de Outliers por Reacciones
# --------------------------------
st.header("Detecci√≥n de Outliers por Reacciones")

if len(df) > 0 and "Reacciones" in df.columns:
    mean_reacts = df["Reacciones"].mean()
    std_reacts = df["Reacciones"].std()
    threshold = mean_reacts + 2 * std_reacts
    df_outliers = df[df["Reacciones"] > threshold].copy()
    st.markdown(f"**Umbral de outlier (media + 2¬∑desviaci√≥n): {threshold:.2f}**")
    if not df_outliers.empty:
        id_col = "Post_ID" if dataset_option == "Posts" else "Comment_ID"
        st.dataframe(
            df_outliers[[id_col, "Autor", "Fecha", "Reacciones", "Mensaje"]]
            .sort_values(by="Reacciones", ascending=False),
            use_container_width=True
        )
    else:
        st.write("No se encontraron registros fuera del umbral de outliers.")
else:
    st.write("No hay datos suficientes para detectar outliers.")

# --------------------------------
# 12. Generaci√≥n de N‚Äêgramas y c√°lculo de likes
# --------------------------------
st.header(f"Top N‚Äêgramas ({ngram_option})")

textos = df["Mensaje"].dropna().astype(str)
base_stop = list(stopwords.words("spanish"))
extra_stop = [
    "http", "https", "www", "com", "org", "net", "ftp",
    "https://", "http://", "://", "url", "rt"
]
spanish_stopwords = base_stop + extra_stop

@st.cache_data
def generar_ngrams(text_series: pd.Series, ngram_range=(1, 1), likes=None):
    vectorizer = CountVectorizer(
        ngram_range=ngram_range,
        stop_words=spanish_stopwords
    )
    X = vectorizer.fit_transform(text_series)
    ngrams = vectorizer.get_feature_names_out()
    counts = X.sum(axis=0).A1
    df_ngrams = pd.DataFrame({"ngram": ngrams, "count": counts})
    if likes is not None:
        likes_array = likes.values
        likes_sum = X.T.dot(likes_array)
        df_ngrams["likes_sum"] = likes_sum
    return df_ngrams.sort_values(by="count", ascending=False)

likes_param = df["Reacciones"] if use_likes else None
df_ngrams = generar_ngrams(textos, ngram_range=ngram_range, likes=likes_param)

if use_likes:
    st.markdown("*Ordenado por frecuencia de aparici√≥n*")
    st.dataframe(df_ngrams[["ngram", "count", "likes_sum"]].head(50), use_container_width=True)
    st.markdown("**Para ordenar por m√©tricas de likes, haz clic en la cabecera 'likes_sum' en la tabla.**")
else:
    st.dataframe(df_ngrams[["ngram", "count"]].head(50), use_container_width=True)

# --------------------------------
# 13. Tendencias temporales de N‚Äêgrama (opci√≥n en sidebar)
# --------------------------------
show_trend = st.sidebar.checkbox("Mostrar tendencias temporales de un N‚Äêgrama", value=False)
if show_trend:
    ngram_for_trend = st.sidebar.text_input("Introduce un N‚Äêgrama para su tendencia (exacto)", "")
    if ngram_for_trend.strip() != "":
        st.subheader(f"Tendencia Temporal para el N‚Äêgrama: '{ngram_for_trend}'")
        df_temp = df.copy()
        df_temp['texto_lower'] = df_temp['Mensaje'].str.lower().fillna('')
        pattern = rf"\b{re.escape(ngram_for_trend.lower())}\b"
        df_temp['match_ngram'] = df_temp['texto_lower'].apply(lambda t: bool(re.search(pattern, t)))
        trend_series = df_temp.groupby(df_temp['Fecha'].dt.date)['match_ngram'].sum()
        fecha_index = pd.date_range(start=fecha_inicio, end=fecha_fin)
        trend_series = trend_series.reindex(fecha_index.date, fill_value=0)
        st.line_chart(trend_series)

# --------------------------------
# 14. Word Cloud (opci√≥n en sidebar)
# --------------------------------
show_wordcloud = st.sidebar.checkbox("Mostrar Word Cloud", value=False)
if show_wordcloud:
    st.subheader("Word Cloud del Conjunto Filtrado")
    combined_text = " ".join(textos)
    wc = WordCloud(
        width=800, height=400,
        background_color="white",
        stopwords=set(spanish_stopwords)
    ).generate(combined_text)
    fig_wc, ax_wc = plt.subplots(figsize=(10, 5))
    ax_wc.imshow(wc, interpolation="bilinear")
    ax_wc.axis("off")
    st.pyplot(fig_wc)

# --------------------------------
# 15. Topic Modeling LDA (opci√≥n en sidebar) - CORREGIDO
# --------------------------------
show_topics = st.sidebar.checkbox("Mostrar Topic Modeling", value=False)
if show_topics:
    num_topics = st.sidebar.slider("N√∫mero de t√≥picos LDA", min_value=2, max_value=10, value=3)
    st.subheader(f"Topic Modeling con LDA ({num_topics} t√≥picos)")

    tf_vectorizer = CountVectorizer(
        stop_words=spanish_stopwords,
        max_df=0.95,
        min_df=2
    )
    tf_matrix = tf_vectorizer.fit_transform(textos)

    lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)
    lda.fit(tf_matrix)

    words = tf_vectorizer.get_feature_names_out()
    topics = {}
    for idx, topic in enumerate(lda.components_):
        top_indices = topic.argsort()[-10:][::-1]
        top_words = [words[i] for i in top_indices]
        topics[f"T√≥pico {idx+1}"] = ", ".join(top_words)

    df_topics = pd.DataFrame.from_dict(topics, orient='index', columns=['Palabras Clave'])
    st.write(df_topics)

# --------------------------------
# 16. Correlaci√≥n Sentimiento vs Reacciones (solo Comments)
# --------------------------------
show_corr = st.sidebar.checkbox("Mostrar correlaci√≥n Sentimiento vs Likes (Comments)", value=False)
if dataset_option == "Comments" and show_corr:
    st.subheader("Correlaci√≥n Sentimiento vs Reacciones (Comments)")
    sia = SentimentIntensityAnalyzer()
    df_corr = df.copy()
    df_corr['compound'] = df_corr['Mensaje'].fillna("").apply(lambda t: sia.polarity_scores(str(t))['compound'])
    fig_corr, ax_corr = plt.subplots(figsize=(8, 5))
    ax_corr.scatter(df_corr['compound'], df_corr['Reacciones'], alpha=0.5)
    ax_corr.set_xlabel("Sentimiento (compound)")
    ax_corr.set_ylabel("N√∫mero de Reacciones")
    ax_corr.set_title("Sentimiento vs Reacciones")
    st.pyplot(fig_corr)

# --------------------------------
# 17. Indicaciones finales
# --------------------------------
st.markdown("""
- Puedes analizar Pentagramas adem√°s de uni-, bi-, tri- y cuatrigramas.
- El KPI Dashboard muestra al inicio la visi√≥n general.
- La b√∫squeda en Posts despliega n-gramas de comentarios relacionados.
- Se incorpora detecci√≥n de outliers por reacciones.
- En caso de querer ver tendencias de un n-grama, activa la casilla ‚ÄúMostrar tendencias temporales de un N-grama‚Äù.
- Para Word Cloud, activa ‚ÄúMostrar Word Cloud‚Äù.
- Para Topic Modeling, activa ‚ÄúMostrar Topic Modeling‚Äù y elige el n√∫mero de t√≥picos.
- Para correlaci√≥n Sentimiento vs Reacciones en Comments, activa ‚ÄúMostrar correlaci√≥n Sentimiento vs Likes (Comments)‚Äù.
- Recarga la app si se a√±aden nuevos datos a la base para actualizar rangos y m√©tricas.
""")


