import os
import streamlit as st
import pandas as pd
import pyodbc
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import re
import datetime
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.decomposition import LatentDirichletAllocation
import numpy as np
from pysentimiento import create_analyzer

nltk.download('stopwords')
nltk.download('vader_lexicon')

@st.cache_resource
def get_senti_es():
    return create_analyzer(task="sentiment", lang="es")
sent_analyzer = get_senti_es()

@st.cache_resource
def get_connection():
    server   = os.getenv("DB_SERVER")
    database = os.getenv("DB_NAME")
    user     = os.getenv("DB_USER")
    password = os.getenv("DB_PASSWORD")
    if not all([server, database, user, password]):
        st.error("üî¥ Faltan variables de entorno para la conexi√≥n SQL Server. Revisa los Secrets.")
        st.stop()
    conn_str = (
        "DRIVER={ODBC Driver 17 for SQL Server};"
        f"SERVER={server};"
        f"DATABASE={database};"
        f"UID={user};"
        f"PWD={password}"
    )
    return pyodbc.connect(conn_str)

@st.cache_data
def load_posts():
    conn = get_connection()
    query = "SELECT * FROM Posts"
    df = pd.read_sql(query, conn)
    df['Fecha'] = pd.to_datetime(df['Fecha'])
    df['Post_ID'] = df['Post_ID'].astype(str)
    if 'Reacciones' not in df.columns:
        df['Reacciones'] = 0
    return df[['Post_ID', 'Fecha', 'Autor', 'Mensaje', 'Reacciones']].copy()

@st.cache_data
def load_comments():
    conn = get_connection()
    query = "SELECT * FROM Comments"
    df_c = pd.read_sql(query, conn)
    df_c['Fecha'] = pd.to_datetime(df_c['Fecha'])
    df_c['Post_ID'] = df_c['Post_ID'].astype(str)
    if 'Reacciones' not in df_c.columns:
        df_c['Reacciones'] = 0
    return df_c[['Comment_ID', 'Post_ID', 'Autor', 'Mensaje', 'Fecha', 'Reacciones']].copy()

df_posts = load_posts()
df_comments = load_comments()

@st.cache_data
def categorize_posts(df):
    categoria_dict = {
        "Pol√≠tica": [
            "gobierno", "elecci√≥n", "parlamento", "ley", "ministro",
            "presidente", "alcalde", "congreso", "diputado", "senador",
            "votaci√≥n", "campa√±a", "coalici√≥n", "partido", "oposici√≥n",
            "constituci√≥n","Gonzalo Winter","Gabriel Boric","cosejal","alcalde","luis malla","gobernador","jorge diaz",
            "republicanos","liberales","partido","toha" 
        ],
        "Econom√≠a": [
            "econom√≠a", "inflaci√≥n", "PIB", "mercado", "impuestos",
            "√≠ndice", "inversi√≥n", "banco", "d√≥lar", "bolsa",
            "desempleo", "subsidio", "exportaci√≥n", "importaci√≥n",
            "crisis econ√≥mica", "finanzas"
        ],
        "Salud": [
            "hospital", "vacuna", "pandemia", "covid", "salud",
            "paciente", "enfermero", "doctor", "ministerio de salud",
            "enfermedad", "vacunaci√≥n", "epidemia", "emergencia sanitaria",
            "consultorio", "medicamento"
        ],
        "Educaci√≥n": [
            "escuela", "universidad", "colegio", "profesor", "alumno",
            "matr√≠cula", "prueba", "examen", "curr√≠culo", "docente",
            "taller", "bachillerato", "educaci√≥n superior", "pedagog√≠a"
        ],
        "Seguridad": [
            "delito", "polic√≠a", "robo", "homicidio", "seguridad ciudadana",
            "justicia", "tribunal", "carabinero", "prisi√≥n", "recluso",
            "vigilancia", "crimen", "violencia", "menor infractor",
            "accidente","cerro chu√±o","delicuentes","delicuentes","ladron"
        ],
        "Medio Ambiente": [
            "contaminaci√≥n", "reciclaje", "cambio clim√°tico", "deforestaci√≥n",
            "biodiversidad", "remanente forestal", "vertedero", "energ√≠as renovables",
            "agua", "basura", "aire", "sustentabilidad", "biocombustible"
        ],
        "Tecnolog√≠a": [
            "tecnolog√≠a", "software", "hardware", "inteligencia artificial",
            "innovaci√≥n", "start-up", "aplicaci√≥n", "redes sociales",
            "ciberseguridad", "blockchain", "criptomoneda", "internet",
            "transformaci√≥n digital"
        ],
        "Cultura": [
            "museo", "exposici√≥n", "teatro", "cine", "libro", "arte",
            "concierto", "festival", "danza", "pintura", "escultura",
            "fotograf√≠a", "m√∫sica", "teatro comunitario", "patrimonio"
        ],
        "Deportes": [
            "f√∫tbol", "tenis", "ol√≠mpico", "messi", "torneo", "jugador",
            "selecci√≥n", "liga", "partido", "entrenador", "marat√≥n",
            "juegos", "deporte femenino", "estadio"
        ],
        "Internacional": [
            "onu", "relaciones exteriores", "conflicto", "guerra",
            "tratado", "acuerdo", "embargo", "diplom√°tico", "visita oficial",
            "s√°nchez", "biden", "putin", "xi jinping", "ue", "otan"
        ],
        "Sociedad": [
            "comunidad", "vecino", "manifestaci√≥n", "protesta",
            "derechos humanos", "inmigraci√≥n", "desigualdad", "pobreza",
            "migrante", "ni√±ez", "juventud", "embarazo adolescente",
            "movilidad social"
        ],
        "Otros": []
    }
    def asignar_categoria(texto):
        if not isinstance(texto, str):
            return "Otros"
        texto_min = texto.lower()
        for cat, keywords in categoria_dict.items():
            for kw in keywords:
                pattern = rf"\b{re.escape(kw)}\b"
                if re.search(pattern, texto_min):
                    return cat
        return "Otros"
    df['Categoria'] = df['Mensaje'].apply(asignar_categoria)
    return df

df_posts = categorize_posts(df_posts)
df_comments = df_comments.merge(df_posts[['Post_ID', 'Categoria']], on='Post_ID', how='left')
df_comments['Categoria'] = df_comments['Categoria'].fillna("Otros")

st.sidebar.header("Filtros y Opciones")
dataset_option = st.sidebar.selectbox("Seleccionar dataset", ["Posts", "Comments"])
if dataset_option == "Posts":
    categorias = ["Todas"] + sorted(df_posts["Categoria"].unique().tolist())
else:
    categorias = ["Todas"] + sorted(df_comments["Categoria"].unique().tolist())
selected_category = st.sidebar.selectbox("Categor√≠a", categorias)

fixed_start = datetime.date(2025, 3, 1)
max_fecha = datetime.date.today()
fecha_inicio, fecha_fin = st.sidebar.slider(
    "Rango de fechas",
    min_value=fixed_start,
    max_value=max_fecha,
    value=(fixed_start, max_fecha),
    format="YYYY-MM-DD"
)
ngram_option = st.sidebar.selectbox(
    "Tipo de n-grama",
    ["Unigramas", "Bigramas", "Trigramas", "Cuatrigramas", "Pentagramas"]
)
ngram_range = {
    "Unigramas": (1,1),
    "Bigramas": (2,2),
    "Trigramas": (3,3),
    "Cuatrigramas": (4,4),
    "Pentagramas": (5,5)
}[ngram_option]
use_likes = st.sidebar.checkbox("Mostrar m√©trica de Likes (sumatoria)", value=False)
post_search = st.sidebar.text_input("Buscar en Posts", "")

if dataset_option == "Posts":
    df = df_posts.copy()
else:
    df = df_comments.copy()
if selected_category != "Todas":
    df = df[df["Categoria"] == selected_category]
mask = (df['Fecha'].dt.date >= fecha_inicio) & (df['Fecha'].dt.date <= fecha_fin)
df = df.loc[mask]
if dataset_option == "Posts" and post_search.strip() != "":
    df = df[df['Mensaje'].str.contains(post_search, case=False, na=False)]

st.title("An√°lisis Avanzado de N-gramas, Sentimiento y Temas con KPIs")
col1, col2, col3, col4 = st.columns(4)
col1.metric("Total Registros", f"{len(df)}")
avg_reacts = df["Reacciones"].mean() if "Reacciones" in df.columns else 0
col2.metric("Reacciones Promedio", f"{avg_reacts:.2f}")
if dataset_option == "Comments":
    sia = SentimentIntensityAnalyzer()
    df['compound_tmp'] = df['Mensaje'].fillna("").apply(
        lambda t: sia.polarity_scores(str(t))['compound']
    )
    avg_sent = df['compound_tmp'].mean()
    if avg_sent < -0.1:
        status = "üî¥ Negativo"
    elif avg_sent > 0.1:
        status = "üü¢ Positivo"
    else:
        status = "üü† Neutral"
    col3.metric("Sentimiento Promedio", f"{avg_sent:.2f}", status)
else:
    col3.metric("Sentimiento Promedio", "N/A")
num_cat = df['Categoria'].nunique() if 'Categoria' in df.columns else 0
col4.metric("Categor√≠as Activas", f"{num_cat}")

# --------------------------------
# Buscador por ID de Post (siempre visible en modo Posts)
if dataset_option == "Posts":
    st.subheader("Buscar Post por ID")
    post_id_input = st.text_input("Introduce Post ID", key="postid_input")
    if post_id_input.strip() != "":
        pid_str = post_id_input.strip()
        df_found = df_posts[df_posts['Post_ID'] == pid_str]
        if not df_found.empty:
            post_row = df_found.iloc[0]
            st.markdown(f"**Post encontrado:**")
            st.markdown(f"- ID: {post_row['Post_ID']}")
            st.markdown(f"- Autor: {post_row['Autor']}")
            st.markdown(f"- Fecha: {post_row['Fecha'].date()}")
            st.markdown(f"- Mensaje: {post_row['Mensaje']}")
            st.markdown(f"- Reacciones: {post_row['Reacciones']}")
            st.markdown(f"- Categor√≠a: {post_row['Categoria']}")
            ejemplo_url = f"https://facebook.com/{post_row['Post_ID']}"
            st.markdown(f"[Ir al Post]({ejemplo_url})", unsafe_allow_html=True)
            # Opcional: mostrar comentarios relacionados a ese post ID
            df_coms_post = df_comments[df_comments['Post_ID'] == pid_str]
            if not df_coms_post.empty:
                with st.expander("Ver comentarios relacionados a este Post"):
                    st.dataframe(
                        df_coms_post[['Autor', 'Mensaje', 'Fecha', 'Reacciones']],
                        use_container_width=True
                    )
        else:
            st.info("No se encontr√≥ ning√∫n post con ese ID en la base de datos.")

# --------------------------------
if dataset_option == "Posts" and post_search.strip() != "":
    st.subheader("Tabla resumen de Posts encontrados")
    st.dataframe(
        df[["Post_ID", "Autor", "Fecha", "Mensaje", "Reacciones", "Categoria"]].sort_values("Fecha", ascending=False),
        use_container_width=True
    )

# --------------------------------
# N‚Äêgramas y Sentimiento de Comentarios Relacionados a los Posts Encontrados
if dataset_option == "Posts" and post_search.strip() != "":
    st.header("N‚Äêgramas y Sentimiento de Comentarios Relacionados a los Posts Encontrados")
    post_ids = df['Post_ID'].unique().tolist()
    df_related_comments = df_comments[df_comments['Post_ID'].isin(post_ids)].copy()
    if not df_related_comments.empty:
        df_related_comments['Mensaje_Limpio'] = (
            df_related_comments['Mensaje']
            .astype(str)
            .str.replace(r"http\S+|www\S+|https\S+", "", regex=True)
            .str.replace(r"[^\w\s√°√©√≠√≥√∫√±]", "", regex=True)
            .str.strip()
        )
        with st.spinner("Analizando sentimiento de comentarios relacionados..."):
            df_related_comments['Sentimiento'] = df_related_comments['Mensaje_Limpio'].apply(
                lambda txt: sent_analyzer.predict(txt).output if txt else "none"
            )
        # SOLO USAR SENTIMIENTOS VALIDOS EN TODO
        sentiment_validos = ['POS', 'NEU', 'NEG']
        df_valid = df_related_comments[df_related_comments['Sentimiento'].isin(sentiment_validos)]
        senti_counts = df_valid['Sentimiento'].value_counts(normalize=True)
        st.info("**Distribuci√≥n de Sentimiento en los comentarios relacionados:**")
        for s, name in zip(['POS', 'NEU', 'NEG'], ['Positivo', 'Neutro', 'Negativo']):
            pct = 100 * senti_counts.get(s, 0)
            st.write(f"{name}: {pct:.1f}%")
        # Gr√°fico de torta
        senti_counts_abs = df_valid['Sentimiento'].value_counts()
        labels = ['Positivo', 'Neutro', 'Negativo']
        keys = ['POS', 'NEU', 'NEG']
        values = [senti_counts_abs.get(k, 0) for k in keys]
        colors = ['#3ad29f', '#b6bbc4', '#f34f4f']
        fig, ax = plt.subplots()
        wedges, texts, autotexts = ax.pie(
            values,
            labels=labels,
            autopct='%1.1f%%',
            startangle=90,
            colors=colors
        )
        ax.axis('equal')
        plt.title('Distribuci√≥n de Sentimiento en Comentarios')
        st.pyplot(fig)
        # N-gramas
        base_stop = list(stopwords.words("spanish"))
        extra_stop = [
            "http", "https", "www", "com", "org", "net", "ftp",
            "https://", "http://", "://", "url", "rt"
        ]
        spanish_stopwords = base_stop + extra_stop
        @st.cache_data
        def gen_ngrams_relat(text_series: pd.Series, ngram_range=(1, 1)):
            vectorizer = CountVectorizer(
                ngram_range=ngram_range,
                stop_words=spanish_stopwords
            )
            X = vectorizer.fit_transform(text_series)
            ngrams = vectorizer.get_feature_names_out()
            counts = X.sum(axis=0).A1
            return pd.DataFrame({"ngram": ngrams, "count": counts}).sort_values(by="count", ascending=False)
        df_ngrams_rel = gen_ngrams_relat(df_valid['Mensaje_Limpio'].dropna().astype(str), ngram_range=ngram_range)
        st.subheader(f"Top N‚Äêgramas en Comentarios relacionados ({ngram_option})")
        st.dataframe(df_ngrams_rel.head(50), use_container_width=True)
        # Tabla expandible
        with st.expander("Ver comentarios relacionados y su calificaci√≥n de sentimiento"):
            st.dataframe(
                df_valid[['Autor', 'Mensaje', 'Sentimiento', 'Fecha', 'Reacciones']],
                use_container_width=True
            )
    else:
        st.write("No hay comentarios relacionados con los posts encontrados.")

# --------------------------------
st.header("Detecci√≥n de Outliers por Reacciones")
if len(df) > 0 and "Reacciones" in df.columns:
    mean_reacts = df["Reacciones"].mean()
    std_reacts = df["Reacciones"].std()
    threshold = mean_reacts + 2 * std_reacts
    df_outliers = df[df["Reacciones"] > threshold].copy()
    st.markdown(f"**Umbral de outlier (media + 2¬∑desviaci√≥n): {threshold:.2f}**")
    if not df_outliers.empty:
        id_col = "Post_ID" if dataset_option == "Posts" else "Comment_ID"
        st.dataframe(
            df_outliers[[id_col, "Autor", "Fecha", "Reacciones", "Mensaje"]]
            .sort_values(by="Reacciones", ascending=False),
            use_container_width=True
        )
    else:
        st.write("No se encontraron registros fuera del umbral de outliers.")
else:
    st.write("No hay datos suficientes para detectar outliers.")

st.header(f"Top N‚Äêgramas ({ngram_option})")
textos = df["Mensaje"].dropna().astype(str)
base_stop = list(stopwords.words("spanish"))
extra_stop = [
    "http", "https", "www", "com", "org", "net", "ftp",
    "https://", "http://", "://", "url", "rt"
]
spanish_stopwords = base_stop + extra_stop
@st.cache_data
def generar_ngrams(text_series: pd.Series, ngram_range=(1, 1), likes=None):
    vectorizer = CountVectorizer(
        ngram_range=ngram_range,
        stop_words=spanish_stopwords
    )
    X = vectorizer.fit_transform(text_series)
    ngrams = vectorizer.get_feature_names_out()
    counts = X.sum(axis=0).A1
    df_ngrams = pd.DataFrame({"ngram": ngrams, "count": counts})
    if likes is not None:
        likes_array = likes.values
        likes_sum = X.T.dot(likes_array)
        df_ngrams["likes_sum"] = likes_sum
    return df_ngrams.sort_values(by="count", ascending=False)
likes_param = df["Reacciones"] if use_likes else None
df_ngrams = generar_ngrams(textos, ngram_range=ngram_range, likes=likes_param)
if use_likes:
    st.markdown("*Ordenado por frecuencia de aparici√≥n*")
    st.dataframe(df_ngrams[["ngram", "count", "likes_sum"]].head(50), use_container_width=True)
    st.markdown("**Para ordenar por m√©tricas de likes, haz clic en la cabecera 'likes_sum' en la tabla.**")
else:
    st.dataframe(df_ngrams[["ngram", "count"]].head(50), use_container_width=True)

show_trend = st.sidebar.checkbox("Mostrar tendencias temporales de un N‚Äêgrama", value=False)
if show_trend:
    ngram_for_trend = st.sidebar.text_input("Introduce un N‚Äêgrama para su tendencia (exacto)", "")
    if ngram_for_trend.strip() != "":
        st.subheader(f"Tendencia Temporal para el N‚Äêgrama: '{ngram_for_trend}'")
        df_temp = df.copy()
        df_temp['texto_lower'] = df_temp['Mensaje'].str.lower().fillna('')
        pattern = rf"\b{re.escape(ngram_for_trend.lower())}\b"
        df_temp['match_ngram'] = df_temp['texto_lower'].apply(lambda t: bool(re.search(pattern, t)))
        trend_series = df_temp.groupby(df_temp['Fecha'].dt.date)['match_ngram'].sum()
        fecha_index = pd.date_range(start=fecha_inicio, end=fecha_fin)
        trend_series = trend_series.reindex(fecha_index.date, fill_value=0)
        st.line_chart(trend_series)

show_wordcloud = st.sidebar.checkbox("Mostrar Word Cloud", value=False)
if show_wordcloud:
    st.subheader("Word Cloud del Conjunto Filtrado")
    combined_text = " ".join(textos)
    wc = WordCloud(
        width=800, height=400,
        background_color="white",
        stopwords=set(spanish_stopwords)
    ).generate(combined_text)
    fig_wc, ax_wc = plt.subplots(figsize=(10, 5))
    ax_wc.imshow(wc, interpolation="bilinear")
    ax_wc.axis("off")
    st.pyplot(fig_wc)

show_topics = st.sidebar.checkbox("Mostrar Topic Modeling", value=False)
if show_topics:
    num_topics = st.sidebar.slider("N√∫mero de t√≥picos LDA", min_value=2, max_value=10, value=3)
    st.subheader(f"Topic Modeling con LDA ({num_topics} t√≥picos)")
    tf_vectorizer = CountVectorizer(
        stop_words=spanish_stopwords,
        max_df=0.95,
        min_df=2
    )
    tf_matrix = tf_vectorizer.fit_transform(textos)
    lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)
    lda.fit(tf_matrix)
    words = tf_vectorizer.get_feature_names_out()
    topics = {}
    for idx, topic in enumerate(lda.components_):
        top_indices = topic.argsort()[-10:][::-1]
        top_words = [words[i] for i in top_indices]
        topics[f"T√≥pico {idx+1}"] = ", ".join(top_words)
    df_topics = pd.DataFrame.from_dict(topics, orient='index', columns=['Palabras Clave'])
    st.write(df_topics)

show_corr = st.sidebar.checkbox("Mostrar correlaci√≥n Sentimiento vs Likes (Comments)", value=False)
if dataset_option == "Comments" and show_corr:
    st.subheader("Correlaci√≥n Sentimiento vs Reacciones (Comments)")
    sia = SentimentIntensityAnalyzer()
    df_corr = df.copy()
    df_corr['compound'] = df_corr['Mensaje'].fillna("").apply(lambda t: sia.polarity_scores(str(t))['compound'])
    fig_corr, ax_corr = plt.subplots(figsize=(8, 5))
    ax_corr.scatter(df_corr['compound'], df_corr['Reacciones'], alpha=0.5)
    ax_corr.set_xlabel("Sentimiento (compound)")
    ax_corr.set_ylabel("N√∫mero de Reacciones")
    ax_corr.set_title("Sentimiento vs Reacciones")
    st.pyplot(fig_corr)

st.markdown("""
- Puedes analizar Pentagramas adem√°s de uni-, bi-, tri- y cuatrigramas.
- El KPI Dashboard muestra al inicio la visi√≥n general.
- La b√∫squeda en Posts despliega tabla resumen, n-gramas de comentarios relacionados y an√°lisis de sentimiento con gr√°fico de torta (datos consistentes).
- Buscador por ID de Post con enlace y detalles.
- Se incorpora detecci√≥n de outliers por reacciones.
- En caso de querer ver tendencias de un n-grama, activa la casilla ‚ÄúMostrar tendencias temporales de un N-grama‚Äù.
- Para Word Cloud, activa ‚ÄúMostrar Word Cloud‚Äù.
- Para Topic Modeling, activa ‚ÄúMostrar Topic Modeling‚Äù y elige el n√∫mero de t√≥picos.
- Para correlaci√≥n Sentimiento vs Reacciones en Comments, activa ‚ÄúMostrar correlaci√≥n Sentimiento vs Likes (Comments)‚Äù.
- Recarga la app si se a√±aden nuevos datos a la base para actualizar rangos y m√©tricas.
""")
