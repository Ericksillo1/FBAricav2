import os
import streamlit as st
import pandas as pd
import pyodbc
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import re
import datetime
import matplotlib.pyplot as plt
from wordcloud import WordCloud
from sklearn.decomposition import LatentDirichletAllocation
import numpy as np

# Descargar recursos de NLTK (solo la primera vez)
nltk.download('stopwords')
nltk.download('vader_lexicon')

# --------------------------------
# 1. Conexi√≥n a SQL Server usando variables de entorno
# --------------------------------
@st.cache_resource
def get_connection():
    server   = os.getenv("DB_SERVER")
    database = os.getenv("DB_NAME")
    user     = os.getenv("DB_USER")
    password = os.getenv("DB_PASSWORD")

    if not all([server, database, user, password]):
        st.error("üî¥ Faltan variables de entorno para la conexi√≥n SQL Server. Revisa los Secrets.")
        st.stop()

    conn_str = (
        "DRIVER={ODBC Driver 17 for SQL Server};"
        f"SERVER={server};"
        f"DATABASE={database};"
        f"UID={user};"
        f"PWD={password}"
    )
    return pyodbc.connect(conn_str)

# --------------------------------
# 2. Cargar y procesar Posts
# --------------------------------
@st.cache_data
def load_posts():
    conn = get_connection()
    query = "SELECT * FROM Posts"
    df = pd.read_sql(query, conn)
    df['Fecha'] = pd.to_datetime(df['Fecha'])
    if 'Reacciones' not in df.columns:
        df['Reacciones'] = 0
    return df[['Post_ID', 'Fecha', 'Autor', 'Mensaje', 'Reacciones']].copy()

# --------------------------------
# 3. Cargar y procesar Comments
# --------------------------------
@st.cache_data
def load_comments():
    conn = get_connection()
    query = "SELECT * FROM Comments"
    df_c = pd.read_sql(query, conn)
    df_c['Fecha'] = pd.to_datetime(df_c['Fecha'])
    if 'Reacciones' not in df_c.columns:
        df_c['Reacciones'] = 0
    return df_c[['Comment_ID', 'Post_ID', 'Autor', 'Mensaje', 'Fecha', 'Reacciones']].copy()

# --------------------------------
# 4. Cargar DataFrames
# --------------------------------
df_posts = load_posts()
df_comments = load_comments()

# --------------------------------
# 5. Funci√≥n para categorizar Posts con diccionario ampliado
# --------------------------------
@st.cache_data
def categorize_posts(df):
    categoria_dict = {
        "Pol√≠tica": [
            "gobierno", "elecci√≥n", "parlamento", "ley", "ministro",
            "presidente", "alcalde", "congreso", "diputado", "senador",
            "votaci√≥n", "campa√±a", "coalici√≥n", "partido", "oposici√≥n",
            "constituci√≥n"
        ],
        "Econom√≠a": [
            "econom√≠a", "inflaci√≥n", "PIB", "mercado", "impuestos",
            "√≠ndice", "inversi√≥n", "banco", "d√≥lar", "bolsa",
            "desempleo", "subsidio", "exportaci√≥n", "importaci√≥n",
            "crisis econ√≥mica", "finanzas"
        ],
        "Salud": [
            "hospital", "vacuna", "pandemia", "covid", "salud",
            "paciente", "enfermero", "doctor", "ministerio de salud",
            "enfermedad", "vacunaci√≥n", "epidemia", "emergencia sanitaria",
            "consultorio", "medicamento"
        ],
        "Educaci√≥n": [
            "escuela", "universidad", "colegio", "profesor", "alumno",
            "matr√≠cula", "prueba", "examen", "curr√≠culo", "docente",
            "taller", "bachillerato", "educaci√≥n superior", "pedagog√≠a"
        ],
        "Seguridad": [
            "delito", "polic√≠a", "robo", "homicidio", "seguridad ciudadana",
            "justicia", "tribunal", "carabinero", "prisi√≥n", "recluso",
            "vigilancia", "crimen", "violencia", "menor infractor",
            "accidente"
        ],
        "Medio Ambiente": [
            "contaminaci√≥n", "reciclaje", "cambio clim√°tico", "deforestaci√≥n",
            "biodiversidad", "remanente forestal", "vertedero", "energ√≠as renovables",
            "agua", "basura", "aire", "sustentabilidad", "biocombustible"
        ],
        "Tecnolog√≠a": [
            "tecnolog√≠a", "software", "hardware", "inteligencia artificial",
            "innovaci√≥n", "start-up", "aplicaci√≥n", "redes sociales",
            "ciberseguridad", "blockchain", "criptomoneda", "internet",
            "transformaci√≥n digital"
        ],
        "Cultura": [
            "museo", "exposici√≥n", "teatro", "cine", "libro", "arte",
            "concierto", "festival", "danza", "pintura", "escultura",
            "fotograf√≠a", "m√∫sica", "teatro comunitario", "patrimonio"
        ],
        "Deportes": [
            "f√∫tbol", "tenis", "ol√≠mpico", "messi", "torneo", "jugador",
            "selecci√≥n", "liga", "partido", "entrenador", "marat√≥n",
            "juegos", "deporte femenino", "estadio"
        ],
        "Internacional": [
            "onu", "relaciones exteriores", "conflicto", "guerra",
            "tratado", "acuerdo", "embargo", "diplom√°tico", "visita oficial",
            "s√°nchez", "biden", "putin", "xi jinping", "ue", "otan"
        ],
        "Sociedad": [
            "comunidad", "vecino", "manifestaci√≥n", "protesta",
            "derechos humanos", "inmigraci√≥n", "desigualdad", "pobreza",
            "migrante", "ni√±ez", "juventud", "embarazo adolescente",
            "movilidad social"
        ],
        "Otros": []
    }

    def asignar_categoria(texto):
        if not isinstance(texto, str):
            return "Otros"
        texto_min = texto.lower()
        for cat, keywords in categoria_dict.items():
            for kw in keywords:
                pattern = rf"\b{re.escape(kw)}\b"
                if re.search(pattern, texto_min):
                    return cat
        return "Otros"

    df['Categoria'] = df['Mensaje'].apply(asignar_categoria)
    return df

df_posts = categorize_posts(df_posts)

# 5.1 Asignar categor√≠a a Comments (basado en categor√≠a del Post padre)
df_comments = df_comments.merge(df_posts[['Post_ID', 'Categoria']], on='Post_ID', how='left')
df_comments['Categoria'] = df_comments['Categoria'].fillna("Otros")

# --------------------------------
# 6. Sidebar: selecci√≥n de dataset, categor√≠a, rango de fechas, n-gramas y opciones
# --------------------------------
st.sidebar.header("Filtros y Opciones")

# 6.1. Selector de dataset
dataset_option = st.sidebar.selectbox("Seleccionar dataset", ["Posts", "Comments"])

# 6.2. Selector de categor√≠a
if dataset_option == "Posts":
    categorias = ["Todas"] + sorted(df_posts["Categoria"].unique().tolist())
else:
    categorias = ["Todas"] + sorted(df_comments["Categoria"].unique().tolist())
selected_category = st.sidebar.selectbox("Categor√≠a", categorias)

# 6.3. Slider de rango de fechas
if dataset_option == "Posts":
    min_fecha = df_posts['Fecha'].min().date()
    max_fecha = df_posts['Fecha'].max().date()
else:
    min_fecha = df_comments['Fecha'].min().date()
    max_fecha = df_comments['Fecha'].max().date()
fecha_inicio, fecha_fin = st.sidebar.slider(
    "Rango de fechas",
    min_value=min_fecha,
    max_value=max_fecha,
    value=(min_fecha, max_fecha),
    format="YYYY-MM-DD"
)

# 6.4. Selector de tipo de n-grama
ngram_option = st.sidebar.selectbox("Tipo de n-grama", ["Unigramas", "Bigramas", "Trigramas"])
if ngram_option == "Unigramas":
    ngram_range = (1, 1)
elif ngram_option == "Bigramas":
    ngram_range = (2, 2)
else:
    ngram_range = (3, 3)

# 6.5. Checkbox para usar m√©tricas de Likes
use_likes = st.sidebar.checkbox("Mostrar m√©trica de Likes (sumatoria)", value=False)

# 6.6. Input para b√∫squeda de palabras clave en Mensaje
keyword_search = st.sidebar.text_input("Buscar en Mensaje", "")

# --------------------------------
# 7. Filtrado seg√∫n selecci√≥n
# --------------------------------
if dataset_option == "Posts":
    df = df_posts.copy()
else:
    df = df_comments.copy()

# Filtrar por categor√≠a
if selected_category != "Todas":
    df = df[df["Categoria"] == selected_category]

# Filtrar por fechas
mask = (df['Fecha'].dt.date >= fecha_inicio) & (df['Fecha'].dt.date <= fecha_fin)
df = df.loc[mask]

# Filtrar por palabra clave en Mensaje
if keyword_search.strip() != "":
    df = df[df['Mensaje'].str.contains(keyword_search, case=False, na=False)]

# --------------------------------
# 8. KPI Dashboard
# --------------------------------
st.title("An√°lisis Avanzado de N-gramas, Sentimiento y Temas con KPIs")

col1, col2, col3, col4 = st.columns(4)
col1.metric("Total Registros", f"{len(df)}")
avg_reacts = df["Reacciones"].mean() if "Reacciones" in df.columns else 0
col2.metric("Reacciones Promedio", f"{avg_reacts:.2f}")

if dataset_option == "Comments":
    sia = SentimentIntensityAnalyzer()
    df['compound_tmp'] = df['Mensaje'].fillna("").apply(lambda t: sia.polarity_scores(str(t))['compound'])
    avg_sent = df['compound_tmp'].mean()
    if avg_sent < -0.1:
        status = "üî¥ Negativo"
    elif avg_sent > 0.1:
        status = "üü¢ Positivo"
    else:
        status = "üü† Neutral"
    col3.metric("Sentimiento Promedio", f"{avg_sent:.2f}", status)
else:
    col3.metric("Sentimiento Promedio", "N/A")

num_cat = df['Categoria'].nunique() if 'Categoria' in df.columns else 0
col4.metric("Categor√≠as Activas", f"{num_cat}")

# --------------------------------
# 9. Buscar comentarios por palabra clave (solo si se selecciona ‚ÄúComments‚Äù)
# --------------------------------
if dataset_option == "Comments":
    st.subheader("Buscar Comentarios por Palabra Clave")
    search_term_comments = st.text_input("Palabra clave en Comentarios", "")
    if search_term_comments.strip() != "":
        df_search = df_comments[
            df_comments['Mensaje'].str.contains(search_term_comments, case=False, na=False)
        ].copy()
        if not df_search.empty:
            st.dataframe(
                df_search[["Comment_ID", "Post_ID", "Autor", "Fecha", "Reacciones", "Mensaje"]],
                use_container_width=True
            )
        else:
            st.write("No se encontraron comentarios que contengan esa palabra.")

# --------------------------------
# 10. Detectar y mostrar outliers por reacciones
# --------------------------------
st.header("Detecci√≥n de Outliers por Reacciones")

if len(df) > 0 and "Reacciones" in df.columns:
    mean_reacts = df["Reacciones"].mean()
    std_reacts = df["Reacciones"].std()
    threshold = mean_reacts + 2 * std_reacts
    df_outliers = df[df["Reacciones"] > threshold].copy()
    st.markdown(f"**Umbral de outlier (media + 2¬∑desviaci√≥n): {threshold:.2f}**")
    if not df_outliers.empty:
        id_col = "Post_ID" if dataset_option == "Posts" else "Comment_ID"
        st.dataframe(
            df_outliers[[id_col, "Autor", "Fecha", "Reacciones", "Mensaje"]]
            .sort_values(by="Reacciones", ascending=False),
            use_container_width=True
        )
    else:
        st.write("No se encontraron registros fuera del umbral de outliers.")
else:
    st.write("No hay datos suficientes para detectar outliers.")

# --------------------------------
# 11. Generaci√≥n de N-gramas y c√°lculo de likes
# --------------------------------
st.header(f"Top N-gramas ({ngram_option})")

textos = df["Mensaje"].dropna().astype(str)
base_stop = list(stopwords.words("spanish"))
extra_stop = [
    "http", "https", "www", "com", "org", "net", "ftp",
    "https://", "http://", "://", "url", "rt"
]
spanish_stopwords = base_stop + extra_stop

@st.cache_data
def generar_ngrams(text_series: pd.Series, ngram_range=(1, 1), likes=None):
    vectorizer = CountVectorizer(
        ngram_range=ngram_range,
        stop_words=spanish_stopwords
    )
    X = vectorizer.fit_transform(text_series)
    ngrams = vectorizer.get_feature_names_out()
    counts = X.sum(axis=0).A1
    df_ngrams = pd.DataFrame({"ngram": ngrams, "count": counts})
    if likes is not None:
        likes_array = likes.values
        likes_sum = X.T.dot(likes_array)
        df_ngrams["likes_sum"] = likes_sum
    return df_ngrams.sort_values(by="count", ascending=False)

likes_param = df["Reacciones"] if use_likes else None
df_ngrams = generar_ngrams(textos, ngram_range=ngram_range, likes=likes_param)

if use_likes:
    st.markdown("*Ordenado por frecuencia de aparici√≥n*")
    st.dataframe(df_ngrams[["ngram", "count", "likes_sum"]].head(50), use_container_width=True)
    st.markdown("**Para ordenar por m√©tricas de likes, haz clic en la cabecera 'likes_sum' en la tabla.**")
else:
    st.dataframe(df_ngrams[["ngram", "count"]].head(50), use_container_width=True)

# --------------------------------
# 12. Tendencias temporales de N-grama (opci√≥n en sidebar)
# --------------------------------
show_trend = st.sidebar.checkbox("Mostrar tendencias temporales de un N-grama", value=False)
if show_trend:
    ngram_for_trend = st.sidebar.text_input("Introduce un N-grama para su tendencia (exacto)", "")
    if ngram_for_trend.strip() != "":
        st.subheader(f"Tendencia Temporal para el N-grama: '{ngram_for_trend}'")
        df_temp = df.copy()
        df_temp['texto_lower'] = df_temp['Mensaje'].str.lower().fillna('')
        pattern = rf"\b{re.escape(ngram_for_trend.lower())}\b"
        df_temp['match_ngram'] = df_temp['texto_lower'].apply(lambda t: bool(re.search(pattern, t)))
        trend_series = df_temp.groupby(df_temp['Fecha'].dt.date)['match_ngram'].sum()
        fecha_index = pd.date_range(start=fecha_inicio, end=fecha_fin)
        trend_series = trend_series.reindex(fecha_index.date, fill_value=0)
        st.line_chart(trend_series)

# --------------------------------
# 13. Word Cloud (opci√≥n en sidebar)
# --------------------------------
show_wordcloud = st.sidebar.checkbox("Mostrar Word Cloud", value=False)
if show_wordcloud:
    st.subheader("Word Cloud del Conjunto Filtrado")
    combined_text = " ".join(textos)
    wc = WordCloud(
        width=800, height=400,
        background_color="white",
        stopwords=set(spanish_stopwords)
    ).generate(combined_text)
    fig_wc, ax_wc = plt.subplots(figsize=(10, 5))
    ax_wc.imshow(wc, interpolation="bilinear")
    ax_wc.axis("off")
    st.pyplot(fig_wc)

# --------------------------------
# 14. Topic Modeling LDA (opci√≥n en sidebar)
# --------------------------------
show_topics = st.sidebar.checkbox("Mostrar Topic Modeling", value=False)
if show_topics:
    num_topics = st.sidebar.slider("N√∫mero de t√≥picos LDA", min_value=2, max_value=10, value=3)
    st.subheader(f"Topic Modeling con LDA ({num_topics} t√≥picos)")
    tf_vectorizer = CountVectorizer(
        stop_words=spanish_stopwords,
        max_df=0.95,
        min_df=2
    )
    tf_matrix = tf_vectorizer.fit_transform(textos)
    lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)
    lda.fit(tf_matrix)
    words = tf_vectorizer.get_feature_names_out()
    topics = {}
    for idx, topic in enumerate(lda.components_):
        top_indices = topic.argsort()[-10:][::-1]
        top_words = [words[i] for i in top_indices]
        topics[f"T√≥pico {idx+1}"] = ", ".join(top_words)
    df_topics = pd.DataFrame.from_dict(topics, orient='index', columns=['Palabras Clave'])
    st.write(df_topics)

# --------------------------------
# 15. Correlaci√≥n Sentimiento vs Reacciones (solo Comments)
# --------------------------------
show_corr = st.sidebar.checkbox("Mostrar correlaci√≥n Sentimiento vs Likes (Comments)", value=False)
if dataset_option == "Comments" and show_corr:
    st.subheader("Correlaci√≥n Sentimiento vs Reacciones (Comments)")
    sia = SentimentIntensityAnalyzer()
    df_corr = df.copy()
    df_corr['compound'] = df_corr['Mensaje'].fillna("").apply(lambda t: sia.polarity_scores(str(t))['compound'])
    fig_corr, ax_corr = plt.subplots(figsize=(8, 5))
    ax_corr.scatter(df_corr['compound'], df_corr['Reacciones'], alpha=0.5)
    ax_corr.set_xlabel("Sentimiento (compound)")
    ax_corr.set_ylabel("N√∫mero de Reacciones")
    ax_corr.set_title("Sentimiento vs Reacciones")
    st.pyplot(fig_corr)

# --------------------------------
# 16. Indicaciones finales
# --------------------------------
st.markdown("""
- El KPI Dashboard muestra al inicio la visi√≥n general:
  1. Total de registros filtrados.
  2. Reacciones promedio.
  3. Sentimiento promedio (solo para Comments) con sem√°foro de alerta.
  4. Cantidad de categor√≠as activas.
- Se ha ampliado el diccionario de categor√≠as con m√°s t√©rminos.
- Se han a√±adido stopwords adicionales para filtrar URLs en los n-gramas.
- Se incorpora detecci√≥n de outliers por reacciones.
- Se a√±ade b√∫squeda de palabras clave en Mensaje y, en Comments, b√∫squeda espec√≠fica de comentarios.
- En caso de querer ver tendencias de un n-grama, activa la casilla "Mostrar tendencias temporales de un N-grama".
- Para Word Cloud, activa "Mostrar Word Cloud".
- Para Topic Modeling, activa "Mostrar Topic Modeling" y elige el n√∫mero de t√≥picos.
- Para correlaci√≥n Sentimiento vs Reacciones en Comments, activa "Mostrar correlaci√≥n Sentimiento vs Likes (Comments)".
- Recarga la app si se a√±aden nuevos datos a la base para actualizar rangos y m√©tricas.
""")
